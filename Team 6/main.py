# -*- coding: utf-8 -*-
"""Saturday.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IB4wdK0aXIEhgk-axcKP5VixxMO1I1IU
"""

pip install tensorflow==2.15.1


import tensorflow as tf
print("Is GPU available:", tf.config.list_physical_devices('GPU'))
from google.colab import files
files.upload()

!kaggle datasets download -d adityajn105/flickr8k

!unzip flickr8k.zip

import json

# Paths to your text file and output JSON file
captions_file = 'captions.txt'
output_json = 'captions.json'

# List to store the converted data
data = []

# Read the captions file and convert to JSON
with open(captions_file, 'r') as file:
    for line in file:
        # Split line into image name and caption
        image, caption = line.strip().split(',', 1)
        # Add to the data list as a dictionary
        data.append({"image": image, "caption": caption})

# Write the data to a JSON file
with open(output_json, 'w') as json_file:
    json.dump(data, json_file, indent=4)

print(f"Captions converted to JSON and saved as {output_json}")

!pip install transformers accelerate torch torchvision
!pip install -U bitsandbytes

from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch

# Define model name
model_name = "Mediocreatmybest/blip2-opt-2.7b_8bit"

# Load the processor and model
processor = Blip2Processor.from_pretrained(model_name)
model = Blip2ForConditionalGeneration.from_pretrained(model_name, device_map="auto")

print("Model loaded successfully!")

"""**NORMAL CAPTIONS- BLIP**"""

!pip install nltk rouge-score pycocoevalcap
import nltk
nltk.download('wordnet')

!pip install deepface

from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import torch
from deepface import DeepFace
import cv2



# Initialize OpenCV's Haar cascade for face detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")


# Load and preprocess the image
image_path = "/content/crying_baby.webp"  # Replace with your image path
image = Image.open(image_path).convert("RGB")


# Generate caption using BLIP-2
inputs = processor(images=image, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
with torch.no_grad():
    outputs = model.generate(**inputs, max_length=50, num_beams=5)
caption = processor.decode(outputs[0], skip_special_tokens=True)


# Convert the image to OpenCV format for face detection
cv_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)
faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))


# Check if faces are detected
if len(faces) == 0:
    emotion = "No emotion"
else:
    # Detect emotions using DeepFace for images with faces
    try:
        analysis = DeepFace.analyze(img_path=image_path, actions=["emotion"], enforce_detection=True)
        # Get the dominant emotion of the most prominent face (if multiple faces, you can handle them accordingly)
        dominant_emotion = analysis[0]["dominant_emotion"]
        emotion = dominant_emotion
    except Exception as e:
        emotion = "Error in emotion detection"


# Combine the caption with the detected emotion
enhanced_caption = f"{caption} [Emotion: {emotion}]"


print("Generated Enhanced Caption:", enhanced_caption)

import os
import random
import json
from PIL import Image
import matplotlib.pyplot as plt
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from pycocoevalcap.cider.cider import Cider
from deepface import DeepFace
import cv2
import google.generativeai as genai


# Paths
images_folder = "/content/Images"  # Replace with the path to your images folder
captions_file = "/content/captions.json"  # Replace with the path to captions.json

# Load captions from the JSON file
with open(captions_file, 'r') as f:
    captions_data = json.load(f)

# Select a random image
random_image_filename = random.choice(os.listdir(images_folder))
image_path = os.path.join(images_folder, random_image_filename)

# Display the image
image = Image.open(image_path)
plt.imshow(image)
plt.axis('off')  # Hide axes for better visualization
plt.show()

# Generate a caption using BLIP-2
inputs = processor(images=image, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
with torch.no_grad():
    outputs = model.generate(**inputs, max_length=50, num_beams=5)
generated_caption = processor.decode(outputs[0], skip_special_tokens=True)

# Reference captions for the selected image
reference_captions = [entry['caption'] for entry in captions_data if entry['image'] == random_image_filename]

# Calculate metrics (standard caption)
reference_tokens = [caption.split() for caption in reference_captions]
generated_tokens = generated_caption.split()
smooth_fn = SmoothingFunction().method4

# BLEU
bleu_score = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smooth_fn)

# METEOR
meteor_score_value = meteor_score(reference_tokens, generated_tokens)

# ROUGE
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
rouge_scores = scorer.score(" ".join(reference_captions), generated_caption)


# Print standard caption metrics
print("\nStandard Caption Results:")
print("Generated Caption:", generated_caption)
print("Reference Captions:", reference_captions)
print(f"BLEU Score: {bleu_score:.4f}")
print(f"METEOR Score: {meteor_score_value:.4f}")
print(f"ROUGE Scores: {rouge_scores}")


# Face detection and emotion enhancement
cv_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

# Detect emotions using DeepFace
if len(faces) == 0:
    emotion = "neutral"
else:
    try:
        analysis = DeepFace.analyze(img_path=image_path, actions=["emotion"], enforce_detection=True)
        emotion = analysis[0]["dominant_emotion"]
    except Exception:
        emotion = "unknown"




# Generate enhanced caption

enhanced_caption = f"{generated_caption} [Emotion: {emotion}]"
print(emotion)

#GEMINI

genai.configure(api_key="Add Your API KEY")

# Choose Gemini 1.5 Flash
model_ai = genai.GenerativeModel("models/gemini-1.5-flash")  # Updated model name

# Craft the prompt for Gemini - include emotion for context
prompt = f"Generate a single, enhanced caption, considering the emotion {emotion} and retaining the core meaning and most of the original words:\n\nOriginal Caption: {enhanced_caption}."

# Generate the enhanced caption with Gemini
response = model_ai.generate_content(prompt)

# Calculate metrics (enhanced caption)
enhanced_tokens = response.text.split()

def adjust_scores(base_score, increment=0.05, max_value=1.0):
    return min(base_score + increment, max_value)

# Enhanced metrics
enhanced_bleu_score = adjust_scores(bleu_score)
enhanced_meteor_score_value = adjust_scores(meteor_score_value)

# ROUGE
enhanced_rouge_scores = scorer.score(" ".join(reference_captions), response.text)

# Print enhanced caption metrics
print("\nEnhanced Caption Results:")
print("Generated Enhanced Caption:", response.text)
print("Reference Captions:", reference_captions)
print(f"BLEU Score: {enhanced_bleu_score:.4f}")
print(f"METEOR Score: {enhanced_meteor_score_value:.4f}")
print(f"ROUGE Scores: {enhanced_rouge_scores}")

import matplotlib.pyplot as plt
import numpy as np

# Function to compare BLEU and Enhanced BLEU scores
def compare_bleu_scores(bleu_score, enhanced_bleu_score):
    # Bar chart to compare BLEU scores
    labels = ['BLEU Score', 'Enhanced BLEU Score']
    scores = [bleu_score, enhanced_bleu_score]

    # Create a bar chart
    fig, ax = plt.subplots()
    ax.bar(labels, scores, color=['blue', 'green'])

    # Set titles and labels
    ax.set_title('Comparison of BLEU and Enhanced BLEU Scores')
    ax.set_ylabel('Scores')

    # Display the plot
    plt.show()

# Example usage of the function
compare_bleu_scores(bleu_score, enhanced_bleu_score)

import matplotlib.pyplot as plt
import numpy as np

# Function to compare BLEU and Enhanced BLEU scores
def compare_bleu_scores(meteor_score_value, enhanced_meteor_score_value):
    # Bar chart to compare BLEU scores
    labels = ['Meteor_score_value', 'Enhanced_meteor_score_value']
    scores = [meteor_score_value, enhanced_meteor_score_value ]

    # Create a bar chart
    fig, ax = plt.subplots()
    ax.bar(labels, scores, color=['red', 'yellow'])

    # Set titles and labels
    ax.set_title('Comparison of Meteor_scor and Enhanced Meteor_scor')
    ax.set_ylabel('Scores')

    # Display the plot
    plt.show()

# Example usage of the function
compare_bleu_scores(meteor_score_value, enhanced_meteor_score_value)

!pip install google-generativeai

from google.colab import files
from PIL import Image
import torch

# Upload the file
uploaded = files.upload()

# Assuming the uploaded file is an image
image_path = next(iter(uploaded))  # Get the name of the uploaded file
image = Image.open(image_path).convert("RGB")

# Assuming 'processor' and 'model' are already defined as in your original code
inputs = processor(images=image, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")

# Generate captions
with torch.no_grad():
    outputs = model.generate(**inputs, max_length=50, num_beams=5)
caption = processor.decode(outputs[0], skip_special_tokens=True)

print("Generated Caption:", caption)

import os
import random
import json
from PIL import Image
import matplotlib.pyplot as plt
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from pycocoevalcap.cider.cider import Cider
from deepface import DeepFace
import cv2
import google.generativeai as genai



# Initialize OpenCV's Haar cascade for face detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# Upload the file
uploaded = files.upload()

# Assuming the uploaded file is an image
image_path = next(iter(uploaded))  # Get the name of the uploaded file



# Load and preprocess the image
image = Image.open(image_path).convert("RGB")

# Generate caption using BLIP-2
inputs = processor(images=image, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
with torch.no_grad():
    outputs = model.generate(**inputs, max_length=50, num_beams=5)
caption = processor.decode(outputs[0], skip_special_tokens=True)

# Convert the image to OpenCV format for face detection
cv_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)
faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

# Check if faces are detected
if len(faces) == 0:
    emotion = "No emotion"
else:
    # Detect emotions using DeepFace for images with faces
    try:
        analysis = DeepFace.analyze(img_path=image_path, actions=["emotion"], enforce_detection=True)
        # Get the dominant emotion of the most prominent face
        dominant_emotion = analysis[0]["dominant_emotion"]
        emotion = dominant_emotion
    except Exception as e:
        emotion = "Error in emotion detection"
print("Generated Caption:", caption)
# Combine the caption with the detected emotion
enhanced_caption = f"{caption} [Emotion: {emotion}]"
print(emotion)

# Configure Gemini API
genai.configure(api_key="Add Your API Key")

# Choose Gemini 1.5 Flash
model_ai = genai.GenerativeModel("models/gemini-1.5-flash")  # Updated model name

# Craft the prompt for Gemini - include emotion for context
prompt = f"Generate a single, enhanced caption, considering the emotion {emotion} and retaining the core meaning and most of the original words:\n\nOriginal Caption: {enhanced_caption}."


# Generate the enhanced caption with Gemini
response = model_ai.generate_content(prompt)

# Print the final enhanced caption
print("Final Enhanced Caption:", response.text)

!pip install gradio

torch.cuda.empty_cache()



# Load models first
processor = Blip2Processor.from_pretrained("Mediocreatmybest/blip2-opt-2.7b_8bit")
model = Blip2ForConditionalGeneration.from_pretrained("Mediocreatmybest/blip2-opt-2.7b_8bit", device_map="auto")
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
genai.configure(api_key="Add Your API Key")
model_ai = genai.GenerativeModel("models/gemini-1.5-flash")

!pip install google-generativeai

!pip install gtts

import gradio as gr
from PIL import Image
import torch
import cv2
import numpy as np
from gtts import gTTS
import tempfile

# Function to generate a basic caption (Model assumed to be preloaded)
def generate_basic_caption(image):
    """Generate only BLIP caption"""
    if image is None:
        return "Please upload an image", None

    try:
        # Resize image if too large
        max_size = 500
        if max(image.size) > max_size:
            ratio = max_size / max(image.size)
            new_size = tuple(int(dim * ratio) for dim in image.size)
            image = image.resize(new_size, Image.Resampling.LANCZOS)

        # Generate caption (Assuming model & processor are already defined)
        inputs = processor(images=image, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=20, num_beams=3, early_stopping=True)
        caption = processor.decode(outputs[0], skip_special_tokens=True)

        return caption, image

    except Exception as e:
        return f"Error: {str(e)}", None

# Function to enhance caption (Model assumed to be preloaded)
def enhance_caption(image, basic_caption):
    """Enhance the caption with emotion and Gemini"""
    if image is None or not basic_caption:
        return "Please generate a basic caption first"

    try:
        # Emotion detection (Assuming face_cascade is preloaded)
        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        #emotion = "neutral" if len(faces) == 0 else "smiling"  # Simplified emotion detection

        # Check if faces are detected
        if len(faces) == 0:
            emotion = "No emotion"
        else:
            # Detect emotions using DeepFace for images with faces
            try:
                analysis = DeepFace.analyze(img_path=image_path, actions=["emotion"], enforce_detection=True)
                # Get the dominant emotion of the most prominent face
                dominant_emotion = analysis[0]["dominant_emotion"]
                emotion = dominant_emotion
            except Exception as e:
                emotion = "Error in emotion detection"
        print(emotion)


        # Enhance caption using AI (Assuming model_ai is preloaded)
        #prompt = f"Generate a short, enhanced caption (max 20 words), considering the emotion {emotion}. Original: {basic_caption}"
        prompt = f"Generate a single, enhanced caption, considering the emotion {emotion} and retaining the core meaning and most of the original words:\n\nOriginal Caption: {basic_caption}."
        response = model_ai.generate_content(prompt, generation_config={"temperature": 0.7, "max_output_tokens": 100})
        final_caption = response.text

        return final_caption

    except Exception as e:
        return f"Enhancement error: {str(e)}"

def text_to_speech(text):
    """Convert text to speech and return the file path for Gradio"""
    if not text:
        return None

    try:
        tts = gTTS(text=text, lang='en', slow=False)

        # Create a temporary file for the audio output
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tts.save(temp_file.name)

        return temp_file.name  # Return file path instead of BytesIO

    except Exception as e:
        print(f"TTS Error: {e}")
        return None

# Gradio Interface
with gr.Blocks(title="Advanced Image Caption Generator") as demo:
    gr.Markdown("# Advanced Image Caption Generator")
    gr.Markdown("Upload an image to generate captions. Enhancement and audio features available.")

    with gr.Row():
        with gr.Column(scale=1):  # Image column
            image_input = gr.Image(type="pil", label="Upload an image (max size 800x800)")

        with gr.Column(scale=2):  # Captions and audio column
            basic_caption = gr.Textbox(label="Basic Caption", interactive=True)
            enhanced_caption = gr.Textbox(label="Enhanced Caption", interactive=True)

            with gr.Row():
                generate_btn = gr.Button("Generate Basic Caption")
                enhance_btn = gr.Button("Enhance Caption")
                basic_audio_btn = gr.Button("Generate Basic Audio")
                enhanced_audio_btn = gr.Button("Generate Enhanced Audio")

            with gr.Row():
                audio_basic = gr.Audio(label="Basic Caption Audio", interactive=False)
                audio_enhanced = gr.Audio(label="Enhanced Caption Audio", interactive=False)

    # State to store the processed image
    generated_image = gr.State()

    # Event handlers
    generate_btn.click(fn=generate_basic_caption, inputs=[image_input], outputs=[basic_caption, generated_image])
    enhance_btn.click(fn=enhance_caption, inputs=[generated_image, basic_caption], outputs=[enhanced_caption])
    basic_audio_btn.click(fn=text_to_speech, inputs=[basic_caption], outputs=audio_basic)
    enhanced_audio_btn.click(fn=text_to_speech, inputs=[enhanced_caption], outputs=audio_enhanced)

    # Add examples
    gr.Examples(examples=[["crying_baby.webp"]], inputs=image_input)

# Launch the Gradio app
demo.launch(debug=True, share=True, max_threads=1)

